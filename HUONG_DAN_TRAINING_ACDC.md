# HÆ°á»›ng dáº«n Training Swin UNet trÃªn táº­p ACDC

## 1. Chuáº©n bá»‹ dá»¯ liá»‡u

### 1.1 Cáº¥u trÃºc dá»¯ liá»‡u ACDC

Dá»¯ liá»‡u cáº§n Ä‘Æ°á»£c tá»• chá»©c nhÆ° sau:
```
../data/ACDC/
â”œâ”€â”€ ACDC_training_slices/        # File .h5 cá»§a slice (cho training)
â”‚   â”œâ”€â”€ patient001_0.h5
â”‚   â”œâ”€â”€ patient001_1.h5
â”‚   â”œâ”€â”€ ...
â”‚   â””â”€â”€ patient100_10.h5
â””â”€â”€ ACDC_training_volumes/       # File .h5 cá»§a volume (cho validation/test)
    â”œâ”€â”€ patient001.h5
    â”œâ”€â”€ patient002.h5
    â””â”€â”€ ...
    â””â”€â”€ patient100.h5
```

### 1.2 Format file .h5

Má»—i file H5 pháº£i chá»©a 2 key:
- `image`: máº£ng numpy shape `(H, W)` cho slice hoáº·c `(D, H, W)` cho volume
- `label`: máº£ng numpy cÃ³ cÃ¹ng shape, giÃ¡ trá»‹ nhÃ£n (0-3 cho ACDC)

NhÃ£n ACDC:
- 0: Background
- 1: Right Ventricle (RV)
- 2: Myocardium (MYO)
- 3: Left Ventricle (LV)
- 4: (ignored class - dÃ¹ng cho ignore_index trong CrossEntropyLoss)

### 1.3 TÃ¡ch dataset

Dá»¯ liá»‡u Ä‘Æ°á»£c tÃ¡ch tá»± Ä‘á»™ng nhÆ° sau:
- **Training**: patient001 - patient080 (80 ca)
- **Validation**: patient081 - patient090 (10 ca)
- **Testing**: patient091 - patient100 (10 ca)

File `dataset_acdc.py` xá»­ lÃ½ tá»± Ä‘á»™ng sá»± phÃ¢n chia nÃ y thÃ´ng qua hÃ m `_get_ids()`.

---

## 2. CÃ i Ä‘áº·t vÃ  chuáº©n bá»‹ mÃ´i trÆ°á»ng

### 2.1 CÃ i Ä‘áº·t dependencies
```bash
pip install -r requirements.txt
```

**CÃ¡c thÆ° viá»‡n cáº§n:**
- torch, torchvision
- numpy, scipy, h5py
- tensorboard, tensorboardX
- tqdm, timm, einops
- medpy, SimpleITK
- ml-collections

### 2.2 Kiá»ƒm tra GPU
```bash
python -c "import torch; print(torch.cuda.is_available()); print(torch.cuda.get_device_name(0))"
```

---

## 3. Cháº¡y Training

### 3.1 Command cÆ¡ báº£n
```bash
python train_acdc.py --dataset ACDC
```

### 3.2 CÃ¡c tham sá»‘ quan trá»ng

| Tham sá»‘ | GiÃ¡ trá»‹ máº·c Ä‘á»‹nh | MÃ´ táº£ |
|---------|-----------------|--------|
| `--dataset` | ACDC | TÃªn dataset (ACDC hoáº·c Synapse) |
| `--root_path` | ../data/ACDC | ÄÆ°á»ng dáº«n Ä‘áº¿n thÆ° má»¥c dá»¯ liá»‡u |
| `--num_classes` | 4 | Sá»‘ lá»›p (4 cho ACDC) |
| `--img_size` | 224 | KÃ­ch thÆ°á»›c input áº£nh (224x224) |
| `--batch_size` | 24 | Batch size cho training |
| `--max_iterations` | 30000 | Sá»‘ iteration tá»‘i Ä‘a |
| `--max_epochs` | 150 | Sá»‘ epoch tá»‘i Ä‘a |
| `--base_lr` | 0.01 | Learning rate ban Ä‘áº§u |
| `--seed` | 1234 | Random seed |
| `--n_gpu` | 1 | Sá»‘ GPU sá»­ dá»¥ng |
| `--deterministic` | 1 | Sá»­ dá»¥ng deterministic training |

### 3.3 VÃ­ dá»¥ cháº¡y vá»›i tham sá»‘ custom
```bash
# Training vá»›i batch size nhá» hÆ¡n
python train_acdc.py --dataset ACDC --batch_size 12 --max_iterations 20000

# Training vá»›i learning rate khÃ¡c
python train_acdc.py --dataset ACDC --base_lr 0.005 --max_epochs 200

# Training vá»›i seed khÃ¡c (Ä‘á»ƒ test robustness)
python train_acdc.py --dataset ACDC --seed 42 --batch_size 16
```

---

## 4. Cáº¥u trÃºc Code chÃ­nh

### 4.1 File `train_acdc.py`
- **TÃ¡c dá»¥ng**: Script chÃ­nh Ä‘á»ƒ run training
- **Chá»©c nÄƒng**:
  1. Parse arguments tá»« command line
  2. Set deterministic training (seed + cudnn)
  3. Khá»Ÿi táº¡o model Swin UNet
  4. Gá»i hÃ m `trainer_acdc()` Ä‘á»ƒ báº¯t Ä‘áº§u training

### 4.2 File `trainer_acdc.py`
- **HÃ m `trainer_acdc(args, model, snapshot_path)`**:
  - Load dataset tá»« `ACDC_training_slices` (train) vÃ  `ACDC_training_volumes` (val)
  - Setup optimizer (SGD) vÃ  loss functions (CrossEntropy + Dice)
  - VÃ²ng láº·p training vá»›i:
    - Forward pass
    - Calculate loss
    - Backward pass + optimize
    - Learning rate scheduling (poly decay)
    - Validation má»—i 500 iterations
    - Save best model

### 4.3 File `dataset_acdc.py`
- **Class `BaseDataSets`**:
  - Load tá»« file H5
  - Tá»± Ä‘á»™ng tÃ¡ch train/val/test theo patient ID
  - Ãp dá»¥ng augmentation (RandomGenerator)

### 4.4 File `utils.py`
- **`DiceLoss`**: Loss function káº¿t há»£p Dice coefficient
- **`test_single_volume()`**: HÃ m evaluate trÃªn single volume
- **`calculate_metric_percase()`**: TÃ­nh Dice vÃ  HD95 metrics

---

## 5. Output vÃ  Monitoring

### 5.1 Cáº¥u trÃºc thÆ° má»¥c output
```
../model/TU_ACDC224/TU/
â”œâ”€â”€ log.txt                          # Log file
â”œâ”€â”€ log/                             # TensorBoard logs
â”‚   â”œâ”€â”€ events.out.tfevents.*
â””â”€â”€ best_model.pth                  # Best model checkpoint
```

### 5.2 TensorBoard
Äá»ƒ monitor training:
```bash
tensorboard --logdir=../model/TU_ACDC224/TU/log/
```

Sau Ä‘Ã³ má»Ÿ `http://localhost:6006` trÃªn browser.

**CÃ¡c metrics Ä‘Æ°á»£c track:**
- `info/lr`: Learning rate
- `info/total_loss`: Total loss
- `info/loss_ce`: CrossEntropy loss
- `info/val_*_dice`: Dice score cho tá»«ng class
- `info/val_*_hd95`: HD95 metric cho tá»«ng class
- `info/val_mean_dice`: Trung bÃ¬nh Dice trÃªn táº¥t cáº£ classes

### 5.3 Log file content
```
[HH:MM:SS.mmm] iteration 1 : loss : 2.123, loss_ce: 2.456
[HH:MM:SS.mmm] iteration 500 : mean_dice : 0.654 mean_hd95 : 15.23
[HH:MM:SS.mmm] Best model | iteration 500 : mean_dice : 0.654 mean_hd95 : 15.23
```

---

## 6. Training workflow chi tiáº¿t

### Step 1: Data Preparation
```bash
# Äáº·t dá»¯ liá»‡u vÃ o thÆ° má»¥c
mkdir -p ../data/ACDC
# Copy ACDC_training_slices/ vÃ  ACDC_training_volumes/ vÃ o thÆ° má»¥c trÃªn
```

### Step 2: Kiá»ƒm tra paths
```bash
# XÃ¡c nháº­n cáº¥u trÃºc
ls -la ../data/ACDC/ACDC_training_slices/ | head
ls -la ../data/ACDC/ACDC_training_volumes/ | head
```

### Step 3: Cháº¡y training
```bash
# Option 1: Default settings
python train_acdc.py --dataset ACDC

# Option 2: Náº¿u cÃ³ GPU memory issues, giáº£m batch size
python train_acdc.py --dataset ACDC --batch_size 8

# Option 3: Quick test (iteration tháº¥p)
python train_acdc.py --dataset ACDC --max_iterations 1000 --batch_size 12
```

### Step 4: Monitor progress
```bash
# Terminal 1: Xem real-time log
tail -f ../model/TU_ACDC224/TU/log.txt

# Terminal 2: Launch TensorBoard
tensorboard --logdir=../model/TU_ACDC224/TU/log/
```

### Step 5: Inference sau khi training
```bash
python test_acdc.py --model_path ../model/TU_ACDC224/TU/best_model.pth
```

---

## 7. Lá»—i thÆ°á»ng gáº·p vÃ  cÃ¡ch kháº¯c phá»¥c

### 7.1 "FileNotFoundError: .../ACDC_training_slices"
**NguyÃªn nhÃ¢n**: Dá»¯ liá»‡u chÆ°a Ä‘Æ°á»£c Ä‘áº·t Ä‘Ãºng vá»‹ trÃ­
**Giáº£i phÃ¡p**: 
```bash
# Kiá»ƒm tra cáº¥u trÃºc
find ../data/ACDC -type d | head -5
```

### 7.2 "CUDA out of memory"
**Giáº£i phÃ¡p**: Giáº£m batch size
```bash
python train_acdc.py --dataset ACDC --batch_size 4 --img_size 192
```

### 7.3 "No module named 'vit_seg_modeling'"
**NguyÃªn nhÃ¢n**: Import sai module (Ä‘Ã£ Ä‘Æ°á»£c fix trong train_acdc.py)
**Giáº£i phÃ¡p**: File Ä‘Ã£ Ä‘Æ°á»£c sá»­a, dÃ¹ng version hiá»‡n táº¡i

### 7.4 Model khÃ´ng há»™i tá»¥
**Giáº£i phÃ¡p**:
- TÄƒng learning rate: `--base_lr 0.02`
- Giáº£m learning rate: `--base_lr 0.005`
- TÄƒng sá»‘ iterations: `--max_iterations 50000`

---

## 8. Advanced Configuration

### 8.1 Multi-GPU training (náº¿u cÃ³)
```bash
# Hiá»‡n táº¡i code há»— trá»£ single GPU
# Äá»ƒ multi-GPU, cáº§n sá»­a trainer_acdc.py thÃªm:
# if args.n_gpu > 1:
#     model = nn.DataParallel(model)
```

### 8.2 Mixed Precision Training
```python
# ThÃªm vÃ o trainer_acdc.py náº¿u muá»‘n tÄƒng tá»‘c Ä‘á»™:
from torch.cuda.amp import autocast, GradScaler
scaler = GradScaler()

# Trong vÃ²ng training loop:
with autocast():
    outputs = model(volume_batch)
    loss = ...
scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

### 8.3 Custom augmentation
Sá»­a `RandomGenerator` trong `dataset_acdc.py`:
```python
class RandomGenerator(object):
    def __init__(self, output_size, low_res=False):
        self.output_size = output_size

    def __call__(self, sample):
        image = sample['image']
        label = sample['label']
        
        # Custom augmentation here
        
        return {'image': image, 'label': label}
```

---

## 9. Sá»‘ liá»‡u tham kháº£o

### 9.1 Expected Results trÃªn ACDC
Sau ~30k iterations:
- **Dice Score**: ~0.85-0.90
- **HD95**: ~10-15 mm (tÃ¹y class)

### 9.2 Training time
- GPU V100: ~5-6 giá» cho 30k iterations (batch_size=24)
- GPU A100: ~2-3 giá»
- GPU RTX 3090: ~3-4 giá»

---

## 10. CÃ¡c files quan trá»ng tá»« TransUnet_acdc_supplymentary

ThÆ° má»¥c `TransUnet_acdc_supplymentary/` chá»©a file gá»‘c tá»« TransUNet. Nhá»¯ng file nÃ y Ä‘Ã£ Ä‘Æ°á»£c refactor Ä‘á»ƒ:

1. **train_acdc.py**: 
   - Tá»«: DÃ¹ng `vit_seg_modeling` (TransUNet VisionTransformer)
   - Sang: DÃ¹ng `vision_transformer` (Swin UNet)
   - Import Ä‘Æ°á»£c sá»­a Ä‘á»ƒ dÃ¹ng SwinUnet thay vÃ¬ ViT

2. **trainer_acdc.py**:
   - Import `test_single_volume` tá»« utils
   - Fix lá»—i undefined `testloader`
   - ThÃªm logging configuration

3. **dataset_acdc.py**:
   - TÆ°Æ¡ng tá»± giá»¯a TransUNet vÃ  Swin UNet
   - Há»— trá»£ cáº£ slice-based (training) vÃ  volume-based (validation) loading

**Tip**: Náº¿u gáº·p lá»—i, so sÃ¡nh vá»›i files trong `TransUnet_acdc_supplymentary/` Ä‘á»ƒ hiá»ƒu cáº¥u trÃºc gá»‘c.

---

## 11. Checklist trÆ°á»›c khi training

- [ ] Dá»¯ liá»‡u H5 Ä‘Ã£ Ä‘Æ°á»£c chuáº©n bá»‹ vÃ  Ä‘áº·t trong `../data/ACDC/`
- [ ] Dependencies Ä‘Æ°á»£c cÃ i: `pip install -r requirements.txt`
- [ ] GPU kháº£ dá»¥ng: `python -c "import torch; print(torch.cuda.is_available())"`
- [ ] ThÆ° má»¥c model output tá»“n táº¡i hoáº·c script sáº½ táº¡o tá»± Ä‘á»™ng
- [ ] Batch size phÃ¹ há»£p vá»›i GPU memory
- [ ] Learning rate Ä‘Æ°á»£c chá»n há»£p lÃ½
- [ ] TensorBoard Ä‘Æ°á»£c cÃ i: `pip install tensorboard tensorboardX`

---

## 12. Káº¿t luáº­n

Äá»ƒ train Swin UNet trÃªn ACDC:
```bash
# 1. Chuáº©n bá»‹ dá»¯ liá»‡u
mkdir -p ../data/ACDC
# Copy ACDC_training_slices vÃ  ACDC_training_volumes

# 2. CÃ i dependencies
pip install -r requirements.txt

# 3. Run training
python train_acdc.py --dataset ACDC --batch_size 24 --max_iterations 30000

# 4. Monitor trÃªn TensorBoard
tensorboard --logdir=../model/TU_ACDC224/TU/log/
```

Happy training! ğŸš€
